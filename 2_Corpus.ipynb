{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg Corpus\n",
    "\n",
    "- A small selection (42,000) of texts from the Project Gutenberg electronic text archive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into Austen's Emma\n",
    "\n",
    "- Just see first 10 words and count the number of word tokens and word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "print(emma_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(emma_words), len(set(emma_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can get it as a long single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_str = gutenberg.raw('austen-emma.txt')\n",
    "print(emma_str[:10])\n",
    "print(len(emma_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or as a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_sents = gutenberg.sents('austen-emma.txt')\n",
    "print(emma_sents[0:3])\n",
    "print(len(emma_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a statistics for each text\n",
    "\n",
    "Print the following three statistics:\n",
    "\n",
    "1. Average number of characters per words\n",
    "2. Average number of words per sentences\n",
    "3. Average number of words per vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in gutenberg.fileids():\n",
    "    n_chars = len(gutenberg.raw(f))\n",
    "    n_words = len(gutenberg.words(f))\n",
    "    n_sents = len(gutenberg.sents(f))\n",
    "    n_vocab = len(set(gutenberg.words(f)))\n",
    "    \n",
    "    avg_word_len = n_chars / n_words\n",
    "    avg_sent_len = n_words / n_sents\n",
    "    avg_frequency = n_words / n_vocab\n",
    "    \n",
    "    print(avg_word_len, avg_sent_len, avg_frequency, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the average sentence length in Milton's Paradise is so long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradise_sents = gutenberg.sents('milton-paradise.txt')\n",
    "\n",
    "maxlen = max([len(s) for s in paradise_sents])\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen, maxsent = max([(len(s), s) for s in paradise_sents])\n",
    "print(maxsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(' '.join(maxsent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "print(chatroom[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus\n",
    "\n",
    "- A corpus created in 1961 at Brown University\n",
    "- An exemplar of the brown corpus\n",
    "- The original brown corpus contains 583 million tokens and 293,181 types\n",
    "- It has been categorized by genre and is convenient for studying differences between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allwords = brown.words(categories=brown.categories())\n",
    "len(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(allwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters Corpus\n",
    "\n",
    "- News corpus in Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.sents('test/14826')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters.categories('test/14826')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(reuters.fileids('trade'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inaugural Address Corpus\n",
    "\n",
    "- First statement by Presidents of U.S.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inaugural.sents('2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are more corpora available\n",
    "\n",
    "Refer to http://www.nltk.org/nltk_data/\n",
    "\n",
    "### Every Corpus in NLTK holds Three Forms of Text\n",
    "\n",
    "- A long single string (raw contents)\n",
    "- A sequence of words\n",
    "- A sequence of sentences (and each sentence is a sequence of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = inaugural.raw('2009-Obama.txt')\n",
    "print(raw[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = inaugural.words('2009-Obama.txt')\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = inaugural.sents('2009-Obama.txt')\n",
    "print(sents[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Text from The Web\n",
    "\n",
    "- `urllib` provides an easy way to access the website and get the contents\n",
    "- However, the web text is usually written in HTML, which contains a lot of tags that specify the layout, the style, images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "url = 'http://www.bbc.com/news/world-us-canada-36439151'\n",
    "html = urlopen(url).read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BeautifulSoup`\n",
    "\n",
    "- `BeautifulSoup` is a library that provides functions to extract contents you want to use\n",
    "- First, we should find the sentences that we want to extract in the HTML\n",
    "- Then, we try to figure out the HTML tags that surround the target sentences\n",
    "  - In this case, `<p></p>` tags seem to be a key structure to characterize the body of the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "ptags = soup.find_all('p')\n",
    "print(ptags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = ' '.join([p.string for p in ptags if p.string is not None])\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems to be good, but some undesired sentences are getting in.\n",
    "  - `\"Share this with Email Facebook Messenger ...\"`\n",
    "  - `\"Cuba has a leader who is not a Castro\"`, which is an anchor text to another news article\n",
    "- So, we look more closely to find the rule to tell them from the target sentence\n",
    "  - In this case, we can find that these messages are surrounded by `p` tags with special attributes such as `<p aria-hidden=\"true\" class=\"twite__title\">`, `<p aria-hidden=\"true\" class=\"twite__channel-text\">`, and `<p class=\"top-stories-promo-story__summary \">`\n",
    "  - Here we try to extract only `p` tags with no class specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptags = soup.find_all('p', class_=None)\n",
    "print(ptags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = ' '.join([p.string for p in ptags if p.string is not None])\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems perfect\n",
    "\n",
    "Now we can analyze the text in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(doc)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(words), len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to process other articles in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_bbc_text(url):\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ptags = soup.find_all('p', class_=None)\n",
    "    return ' '.join([p.string for p in ptags if p.string is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2 = extract_bbc_text('http://www.bbc.com/news/election-us-2016-37918303')\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't make too much access that may be regarded as an attack to the website\n",
    "\n",
    "- Insert `sleep()` for courtecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "urls = ['http://www.bbc.com/news/world-us-canada-36439151', \n",
    "        'http://www.bbc.com/news/election-us-2016-37918303', \n",
    "        'http://www.bbc.com/news/election-us-2016-37468751']\n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.append(extract_bbc_text(url))\n",
    "    sleep(10)\n",
    "    print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([nltk.word_tokenize(d)[:10] for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also you should save the text into a file to avoid a repetitive access to website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = open('bbc.txt', 'w')\n",
    "out.write('\\n'.join(docs))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More information to get to know how to use `BeautifulSoup`\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('bbc.txt')\n",
    "docs = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[len(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [nltk.word_tokenize(d) for d in docs]\n",
    "[len(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
