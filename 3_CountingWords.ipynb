{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data as a collection of documents\n",
    "\n",
    "In this chapter, we represent each document as a list of words.\n",
    "\n",
    "In this way, your own corpus and the corpus provided by NLTK can be processed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('bbc.txt')\n",
    "bbc_docs = [nltk.word_tokenize(line) for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bbc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_docs = [brown.words(fileid) for fileid in brown.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words\n",
    "\n",
    "`nltk.FreqDist` is an object type that computes and preseves frequency of elements in a given collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = nltk.FreqDist(bbc_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case-insensitive count\n",
    "\n",
    "Just make all the words to be in lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = nltk.FreqDist([w.lower() for w in bbc_docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Filtering by Regular Expression\n",
    "\n",
    "`re` library contains functions for processing related with regular expression.\n",
    "\n",
    "The regular expression `[a-zA-Z]+` matches with a string that contains only alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "count = nltk.FreqDist([w.lower() for w in bbc_docs[0] if re.match('[a-zA-Z]+', w)])\n",
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "A list of words that seem not to be relevant to a topic of document, which is enumerated by someone without any theoretical evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sws = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = nltk.FreqDist([w.lower() for w in bbc_docs[0] \n",
    "                       if re.match('[a-zA-Z]+', w) and not w.lower() in sws])\n",
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count['republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count['republicans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "Algorithmic approach that is based on a heuristics\n",
    "\n",
    "- step1 gets rid of plurals and -ed or –ing\n",
    "- step2 maps double suffices to single ones. So -ization ( = -ize plus -ation) maps to -ize etc.\n",
    "- step3 deletes with -ic-, -full, -ness etc.\n",
    "- step4 takes off -ant, -ence etc.\n",
    "- step5 removes a final -e, and changes -ll to –l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "test_tokens = nltk.word_tokenize(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stemmed = [porter.stem(w) for w in test_tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancaster Stemmer (Paice/Husk Stemmer)\n",
    "\n",
    "Fully rule-based approach that uses the externally stored rules. For example, the rules include:\n",
    "\n",
    "- -ied > -y\n",
    "- -ceed > -cess\n",
    "- -eed > -ee\n",
    "- -ed > -\n",
    "- -hood > -\n",
    "- -e > -\n",
    "- -lief > -liev\n",
    "- -if > -\n",
    "- -ing > -\n",
    "- -iag > -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "stemmed = [lancaster.stem(w) for w in test_tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet Lemmatizer\n",
    "\n",
    "A lemmatizer that uses WordNet as a reference dictionary.\n",
    "WordNet is a dictionary that contains information about semantic relationship between words, so that you can use it as a thesaurus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(w) for w in test_tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = nltk.FreqDist([wnl.lemmatize(w.lower()) for w in bbc_docs[0] \n",
    "                       if re.match('[a-zA-Z]+', w) and not w.lower() in sws])\n",
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count['republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count['republicans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    return nltk.FreqDist([wnl.lemmatize(w.lower()) for w in doc\n",
    "                          if re.match('[a-zA-Z]+', w) and not w.lower() in sws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = count_words(brown_docs[0])\n",
    "count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Salient Words by using Likelihood Ratio\n",
    "\n",
    "The important words in a document would be ones that are used frequently in that document but infrequently in *other* documents.\n",
    "The **likelihood ratio** is a quantity that directly reflects this idea.\n",
    "\n",
    "In our case, the likelihood ratio is a ratio of the two likelihoods;\n",
    "- The likelihood of a word $w$ with respect to the probabilistic distribution of word that is estimated by **the given document $d$**, which is denoted by $\\tilde{p}_d(w)$\n",
    "- The likelihood of a word $w$ with respect to the probabilistic distribution of word that pervades **every possible documents in this world**, which is denoted by $p_{all}(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the *general* word distribution\n",
    "\n",
    "We want to know the *true* word distribution $p_{all}$ that the text in this world follows.\n",
    "But we have only limited samples of them, so we try to *estimate* the distribution using the corpus and denote it as $\\tilde{p}_{all}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_docs = brown_docs + bbc_docs\n",
    "all_count = count_words([w for doc in all_docs for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation for $\\tilde{p}_{all}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dist = nltk.MLEProbDist(all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dist.prob('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dist.prob('republican')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation for $\\tilde{p}_{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = count_words(bbc_docs[0])\n",
    "dist = nltk.MLEProbDist(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.prob('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.prob('republican')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Likelihood Ratio $\\frac{\\tilde{p}_{d}(w)}{\\tilde{p}_{all}(w)}$ for $w = $ `republican`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist.prob('republican') / all_dist.prob('republican')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Likelihood Ratio $\\frac{\\tilde{p}_{d}(w)}{\\tilde{p}_{all}(w)}$ for all $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab = all_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratios = [(w, dist.prob(w) / all_dist.prob(w)) for w in all_vocab if count[w] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ratios, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Estimation\n",
    "\n",
    "Laplace Estimation is an estimation of word distribution that just adds 1 to the frequency of every words.\n",
    "This mitigates the *peaky* estimation that is caused by the smallness of the corpus. We can say that this additional count doesn't affect the estimation almost at all if the corpus is sufficiently large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dist = nltk.LaplaceProbDist(all_count, bins=n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dist.prob('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = count_words(bbc_docs[0])\n",
    "dist = nltk.LaplaceProbDist(count, bins=n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.prob('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratios = [(w, dist.prob(w) / all_dist.prob(w)) for w in all_vocab if count[w] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ratios, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
